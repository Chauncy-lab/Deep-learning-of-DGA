{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import language_helpers\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv1d\n",
    "import tflib.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Google Billion Word at http://www.statmt.org/lm-benchmark/ and\n",
    "# fill in the path to the extracted files here!（在此处填写解压缩文件的路径！）\n",
    "\n",
    "#加载一百万Alexa数据\n",
    "DATA_DIR = 'AlexaTop1M_NoSeparate'\n",
    "if len(DATA_DIR) == 0:\n",
    "    #请在gan_language.py中指定数据目录的路径！\n",
    "    raise Exception(\"Please specify path to data directory in gan_language.py!\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64 # Batch size（训练次数）\n",
    "# How many iterations to train for, min value is 1000, Please increase the number of iteration in 1000 units（请以1000为单位增加迭代次数）\n",
    "ITERS = 30000 \n",
    "SEQ_LEN = 32 # Sequence length in characters（序列长度（以字符为单位））\n",
    "DIM = 512 # Model dimensionality. This is fairly slow and overfits, even on\n",
    "          # Billion Word. Consider decreasing for smaller datasets.（模型尺寸。即使是十亿字，这也相当慢且过拟合。考虑减少较小的数据集。）\n",
    "CRITIC_ITERS = 10 # How many critic iterations per generator iteration. We\n",
    "                  # use 10 for the results in the paper, but 5 should work fine\n",
    "                  # as well.（每个生成器就迭代有多少个批评者critic（也即是原始GAN 的判别器）迭代。本文使用10作为结果，但5应该也可以）\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter.（梯度惩罚Lambda超参数）\n",
    "MAX_N_EXAMPLES = 100000 # Max number of data examples to load. If data loading\n",
    "                          # is too slow or takes too much RAM, you can decrease\n",
    "                          # this (at the expense of having less training data). default value is 10000000\n",
    "                          # （要加载的最大数据示例数。如果数据加载太慢或占用过多RAM，则可以减少此操作（以减少训练数据为代价）。默认值为10000000）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 64\n",
      "\tCRITIC_ITERS: 10\n",
      "\tDATA_DIR: AlexaTop1M_NoSeparate\n",
      "\tDIM: 512\n",
      "\tITERS: 30000\n",
      "\tLAMBDA: 10\n",
      "\tMAX_N_EXAMPLES: 100000\n",
      "\tSEQ_LEN: 32\n",
      "loading dataset...\n",
      "('s', 'h', 'a', 'r', 'e', 'i', 't', 'f', 'o', 'r', 'p', 'c', 'c', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 'h', 'a', 'r', 'j', 'a', 'h', '.', 'a', 'c', '.', 'a', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('g', 'i', 'o', 'c', 'h', 'i', '.', 'i', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 'y', 's', 'h', 'l', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'p', 'e', 'n', 't', 'e', 'c', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('h', 'g', 'h', '7', '2', '4', '.', 'i', 'r', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'v', 'c', 'a', 'p', '.', 'i', 'n', 'f', 'o', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('l', 'a', 'v', 'o', 'r', 'o', '2', '4', '.', 'i', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('u', 'f', 'c', '.', 't', 'v', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 't', 'a', 'r', 'a', 'f', 'r', 'i', 'c', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', 'i', 'x', 'f', 'u', 't', 'u', 'r', 'e', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'n', 'c', 'o', 'm', 'e', 'o', 'n', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('f', 'i', 'n', 'n', 'a', 'i', 'r', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('h', 'a', 'r', 'd', 'x', 'x', 'x', 'm', 'o', 'm', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', 'e', 'a', 'r', 's', 'o', 'n', 'e', 'd', '.', 'c', 'o', '.', 'u', 'k', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'o', 'p', 'm', 'i', 'n', 'e', '.', 'i', 'o', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('m', 'y', 'c', 'a', 'd', 'd', 'i', 'e', '.', 'j', 'p', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'a', 'l', 'k', 's', '.', 'b', 'y', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'c', 'o', 'n', 's', 'p', 'e', 'd', 'i', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', 'l', 'a', 'y', 'v', 'o', 'd', '.', 't', 'v', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('e', 'v', 'a', 'n', 's', 'h', 'a', 'l', 's', 'h', 'a', 'w', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'o', 'k', 'y', 'o', 'i', 'n', 'v', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('o', 'p', 'e', 'r', '.', 'r', 'u', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('w', 'h', 'i', 't', 'e', 'l', 'a', 'b', 'e', 'l', 'd', 'a', 't', 'i', 'n', 'g', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('j', 'm', 'y', 's', '1', '6', '8', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'i', 'g', 'i', 't', 'a', 'l', 'm', 'u', 's', 'i', 'c', 'n', 'e', 'w', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('a', 'p', 'a', 'r', 't', 'm', 'e', 'n', 't', 'f', 'i', 'n', 'd', 'e', 'r', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('f', 'u', 'z', 'o', 'k', 'u', '-', 't', 'o', 'w', 'n', 'p', 'a', 'g', 'e', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 't', 'a', 'd', 't', '-', 'b', 'r', 'e', 'm', 'e', 'r', 'h', 'a', 'v', 'e', 'n', '.', 'd', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'n', 'd', 'i', 'e', 'd', 'a', 'y', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('o', 'o', 'y', 'a', 'l', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('g', 'a', 'r', 'e', 'n', 'a', '.', 'r', 'u', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('w', 'a', 'k', 'a', 'y', 'a', 'm', 'a', '.', 'l', 'g', '.', 'j', 'p', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('a', 'c', '-', 'b', 'e', 's', 'a', 'n', 'c', 'o', 'n', '.', 'f', 'r', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('u', 'n', 'w', 'o', 'm', 'e', 'n', '.', 'o', 'r', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('r', 'u', 's', 't', 'o', 'r', 'r', 'e', 'n', 't', 's', '.', 'i', 'n', 'f', 'o', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 's', 'l', 'a', 'm', 't', 'o', 'd', 'a', 'y', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('n', 'e', 'd', 'i', 'r', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'i', 'g', 'i', 't', 'a', 'l', 's', 't', 'o', 'r', 'm', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('u', 's', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', 'a', 't', 'l', 'a', 's', '.', 'o', 'r', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 'e', 'm', 'a', 'a', 'n', '.', 'c', 'a', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'a', 'i', 'l', 'y', 'p', 'r', 'o', 'g', 'r', 'e', 's', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'm', 'o', 'n', 'e', 'y', '.', 'm', 'y', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'e', 'w', 'a', 'h', 'o', 'k', 'i', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('b', 'i', 't', 's', 'f', 'o', 'r', 'c', 'l', 'i', 'c', 'k', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'd', 'o', '.', 'c', 'o', 'm', '.', 't', 'r', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('a', 'd', 'p', 'o', 'l', 'i', 'c', 'e', '.', 'g', 'o', 'v', '.', 'a', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('y', 'o', 'b', 't', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'i', 'z', 'i', 'l', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('f', 'i', 'n', 'v', 'i', 'z', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', 'r', 'o', 'z', 'e', 'n', 'y', '.', 'c', 'z', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'c', 't', 'v', '.', 'u', 'a', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'n', 'o', 'o', 'z', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('b', 'l', 'o', 'g', '-', 'p', 'e', 'l', 'i', 'c', 'u', 'l', 'a', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('b', 'i', 'd', 'v', 'e', 'r', 'd', 'r', 'd', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('o', 'n', 'l', 'i', 'n', 'e', '-', 'l', 'i', 't', 'e', 'r', 'a', 't', 'u', 'r', 'e', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('u', 'n', 'i', '-', 'p', 'a', 'd', 'e', 'r', 'b', 'o', 'r', 'n', '.', 'd', 'e', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('e', 'p', 'i', 'c', 's', 'a', 'h', 'o', 'l', 'i', 'c', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('n', 'i', 'e', 'l', 'i', 't', '.', 'g', 'o', 'v', '.', 'i', 'n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('u', 'p', 'l', 'o', 'o', 'd', 'e', 'r', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('c', 'm', 'o', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('k', 'i', 'n', 'o', 'm', 'o', 'o', 'v', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('c', 'o', 'i', 'n', 'm', 'a', 'r', 'k', 'e', 't', 'c', 'a', 'p', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('m', '1', '.', 'c', 'o', 'm', '.', 's', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'n', 't', 'e', 'r', 'm', 'e', 'd', 'i', 'a', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('e', 'u', 'r', 'o', 'n', 'i', 'c', 's', '.', 'h', 'u', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', 'l', 'a', 'y', 's', 'p', 'o', 'r', 't', '.', 'c', 'c', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('h', 'a', 'r', 'u', 'k', 'a', '-', 'y', 'u', 'm', 'e', 'n', 'o', 'a', 't', 'o', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('f', 'r', 'i', '-', 'g', 'a', 't', 'e', '.', 'o', 'r', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'r', 'o', 'p', 'p', 'a', 'g', 'e', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', 'w', 'c', 'g', 'o', 'v', '.', 'o', 'r', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('j', 'i', 'l', 'l', 'i', 'a', 'n', 'm', 'i', 'c', 'h', 'a', 'e', 'l', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('j', 'o', 'o', 'm', 'l', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'e', 'e', 'o', 'f', 'f', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('c', 'y', 'b', 'e', 'r', 'l', 'i', 'n', 'k', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('m', 'm', 'o', 'g', 'a', '.', 'n', 'e', 't', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('b', 'i', 't', 'v', 'i', 'd', '.', 's', 'x', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', 'n', 't', 'e', 'r', 's', 't', 'a', 't', 'e', 'b', 'a', 't', 't', 'e', 'r', 'i', 'e', 's', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('c', 'e', 'n', 'g', 'a', 'g', 'e', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('b', 'o', 'u', 'n', 't', 'y', 's', 'o', 'u', 'r', 'c', 'e', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 'e', 'r', 'p', 'e', 'n', 's', '.', 'n', 'l', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('e', 's', 'l', 'g', 'a', 'm', 'i', 'n', 'g', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('m', 'y', 'e', 'g', 'y', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('j', 'f', 'c', '.', 'g', 'o', '.', 'j', 'p', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('s', 't', 'r', 'e', 'e', 't', 'e', 'a', 's', 'y', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'e', 'd', 'e', '1', '6', '8', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('e', 'b', 'a', 'r', '.', 'c', 'o', 'm', '.', 'c', 'n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('m', 'o', 'n', 'i', 't', 'o', 'r', 'd', 'a', 'y', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('u', 'k', 'z', 'n', '.', 'a', 'c', '.', 'z', 'a', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('t', 'o', 'u', 'p', 'i', 'e', '.', 'o', 'r', 'g', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', '2', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('p', '3', '0', 'd', 'o', 'w', 'n', 'l', 'o', 'a', 'd', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('d', 'e', 'a', 'l', 's', 't', 'r', 'e', 'e', 't', 'a', 's', 'i', 'a', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('n', 'e', '.', 'g', 'o', 'v', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('i', '-', 'b', 'o', 's', 's', '.', 'c', 'o', '.', 'k', 'r', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('m', 'i', 't', 'e', 'l', 'e', '.', 'e', 's', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('w', 'o', 'r', 'l', 'd', 'b', 'l', 'a', 'z', 'e', '.', 'i', 'n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('c', 'a', 'l', 'c', 'u', 'l', 'a', 't', 'o', 'r', 's', 'o', 'u', 'p', '.', 'c', 'o', 'm', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "('k', 'o', 'k', 'o', 'r', 'o', '.', 'k', 'i', 'r', '.', 'j', 'p', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
      "loaded 100000 lines in dataset\n"
     ]
    }
   ],
   "source": [
    "#locals获取变量，并且大写本地的数据\n",
    "lib.print_model_settings(locals().copy())\n",
    "\n",
    "#数据处理成，\n",
    "# ('o', 'm', 'e', 'g', 'a', 't', 'r', 'a', 'v', 'e', 'l', 'e', 'r', '.', 'i', 'n', 'f', 'o', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ')\n",
    "#形式为32（SEQ_LEN）的元组数据向量，并且加载100000（MAX_N_EXAMPLES）行\n",
    "lines, charmap, inv_charmap = language_helpers.load_dataset(\n",
    "    max_length=SEQ_LEN,\n",
    "    max_n_examples=MAX_N_EXAMPLES,\n",
    "    data_dir=DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return tf.reshape(\n",
    "        tf.nn.softmax(\n",
    "            tf.reshape(logits, [-1, len(charmap)])\n",
    "        ),\n",
    "        tf.shape(logits)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生产噪音\n",
    "def make_noise(shape):\n",
    "    return tf.random_normal(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResBlock(name, inputs):\n",
    "    output = inputs\n",
    "    output = tf.nn.relu(output)\n",
    "    output = lib.ops.conv1d.Conv1D(name+'.1', DIM, DIM, 5, output)\n",
    "    output = tf.nn.relu(output)\n",
    "    output = lib.ops.conv1d.Conv1D(name+'.2', DIM, DIM, 5, output)\n",
    "    return inputs + (0.3*output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(n_samples, prev_outputs=None):\n",
    "    output = make_noise(shape=[n_samples, 128])\n",
    "    output = lib.ops.linear.Linear('Generator.Input', 128, SEQ_LEN*DIM, output)\n",
    "    output = tf.reshape(output, [-1, DIM, SEQ_LEN])\n",
    "    output = ResBlock('Generator.1', output)\n",
    "    output = ResBlock('Generator.2', output)\n",
    "    output = ResBlock('Generator.3', output)\n",
    "    output = ResBlock('Generator.4', output)\n",
    "    output = ResBlock('Generator.5', output)\n",
    "    output = lib.ops.conv1d.Conv1D('Generator.Output', DIM, len(charmap), 1, output)\n",
    "    output = tf.transpose(output, [0, 2, 1])\n",
    "    output = softmax(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(inputs):\n",
    "    output = tf.transpose(inputs, [0,2,1])\n",
    "    output = lib.ops.conv1d.Conv1D('Discriminator.Input', len(charmap), DIM, 1, output)\n",
    "    output = ResBlock('Discriminator.1', output)\n",
    "    output = ResBlock('Discriminator.2', output)\n",
    "    output = ResBlock('Discriminator.3', output)\n",
    "    output = ResBlock('Discriminator.4', output)\n",
    "    output = ResBlock('Discriminator.5', output)\n",
    "    output = tf.reshape(output, [-1, SEQ_LEN*DIM])\n",
    "    output = lib.ops.linear.Linear('Discriminator.Output', SEQ_LEN*DIM, 1, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /myCode/tflib/ops/conv1d.py:93: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NCHW is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NCHW` for data_format is deprecated, use `NCW` instead\n"
     ]
    }
   ],
   "source": [
    "#real_inputs_discrete 是真实样本的占位符\n",
    "real_inputs_discrete = tf.placeholder(tf.int32, shape=[BATCH_SIZE, SEQ_LEN])\n",
    "#负责将real_inputs_discrete 转为onehot编码 \n",
    "real_inputs = tf.one_hot(real_inputs_discrete, len(charmap))\n",
    "# 生成对应数量的伪样本\n",
    "fake_inputs = Generator(BATCH_SIZE)\n",
    "fake_inputs_discrete = tf.argmax(fake_inputs, fake_inputs.get_shape().ndims-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 送进真假样本，初始化判别器，有基本的判别能力\n",
    "disc_real = Discriminator(real_inputs) \n",
    "disc_fake = Discriminator(fake_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判别器和生成器的损失函数\n",
    "#WGAN生成器gen_cost函数（参照原理公式）\n",
    "#WGAN判别器disc_cost函数（参照原理公式）\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "gen_cost = -tf.reduce_mean(disc_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wgan-gp 改进精髓 改进lipschitz\n",
    "# WGAN lipschitz-penalty \n",
    "alpha = tf.random_uniform(\n",
    "    shape=[BATCH_SIZE,1,1], \n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = fake_inputs - real_inputs\n",
    "# interpolates(插样),在真实样本和生成样本之间随机插值，希望这个约束可以“布满”真实样本和生成样本之间的空间\n",
    "interpolates = real_inputs + (alpha*differences)\n",
    "# interpolates就是随机插值采样得到的图像，gradients就是loss中的梯度惩罚项\n",
    "gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "#求梯度的二范数\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1,2]))\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "# wgan-gp的公式：原来wgan公式+惩罚项 公式链接;http://www.twistedwg.com/2018/10/05/GAN_loss_summary.html\n",
    "disc_cost += LAMBDA*gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params = lib.params_with_name('Generator')\n",
    "disc_params = lib.params_with_name('Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判别器和生成器的优化函数\n",
    "# 训练不带正则项的损失函数gen_cost/disc_cost。\n",
    "# 定义训练的目标函数gen_cost/disc_cost，训练次数及训练模型\n",
    "gen_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.999).minimize(gen_cost, var_list=gen_params)\n",
    "disc_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.999).minimize(disc_cost, var_list=disc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset iterator\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        np.random.shuffle(lines)\n",
    "        for i in range(0, len(lines)-BATCH_SIZE+1, BATCH_SIZE):\n",
    "            yield np.array(\n",
    "                [[charmap[c] for c in l] for l in lines[i:i+BATCH_SIZE]], \n",
    "                dtype='int32'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set JSD for n=1: 0.0002892902687692001\n",
      "validation set JSD for n=2: 0.011284768640386435\n",
      "validation set JSD for n=3: 0.07993772080584989\n",
      "validation set JSD for n=4: 0.17677387369452172\n"
     ]
    }
   ],
   "source": [
    "# During training we monitor JS divergence between the true & generated ngram\n",
    "# distributions for n=1,2,3,4. To get an idea of the optimal values, we\n",
    "# evaluate these statistics on a held-out set first.\n",
    "# 在训练过程中，我们监视JS在真实&生成的ngram分布之间的散度，其中n = 1,2,3,4 为了对最佳值有所了解，我们首先对保留集进行评估。\n",
    "\n",
    "# 创立NgramLanguageModel类对象，n=1，2，3，4的2个数组。分别赋予true_char_ngram_lms 与 validation_char_ngram_lms\n",
    "true_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines[10*BATCH_SIZE:], tokenize=False) for i in range(4)]\n",
    "validation_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines[:10*BATCH_SIZE], tokenize=False) for i in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    print ( \"validation set JSD for n={}: {}\".format(i+1, true_char_ngram_lms[i].js_with(validation_char_ngram_lms[i])) )\n",
    "    \n",
    "true_char_ngram_lms = [language_helpers.NgramLanguageModel(i+1, lines, tokenize=False) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Start]\n",
      "    1% Done! [Iteration]:        300 [300x iterations time   ]:     393.32 secs [SUM]:     393.32 secs\n",
      "iter 299\tjs3\t0.23643992584512577\tjs4\t0.29423759144787914\tjs1\t0.05435322801155194\tjs2\t0.14595305449819268\ttrain disc cost\t-3.0827670097351074\ttime\t1.2679182585080464\n",
      "    2% Done! [Iteration]:        600 [300x iterations time   ]:     396.01 secs [SUM]:     789.33 secs\n",
      "iter 599\tjs3\t0.2095511027261185\tjs4\t0.2954987840224037\tjs1\t0.04027807389462579\tjs2\t0.11034715383447981\ttrain disc cost\t-2.4089765548706055\ttime\t1.2691461451848347\n",
      "    3% Done! [Iteration]:        900 [300x iterations time   ]:     395.73 secs [SUM]:    1185.06 secs\n",
      "iter 899\tjs3\t0.19830374120269578\tjs4\t0.286636851346418\tjs1\t0.035703011882826714\tjs2\t0.10012788510265298\ttrain disc cost\t-2.4355621337890625\ttime\t1.2686759209632874\n",
      "    4% Done! [Iteration]:       1200 [300x iterations time   ]:     395.75 secs [SUM]:    1580.81 secs\n",
      "iter 1199\tjs3\t0.178651595212865\tjs4\t0.2628392835344958\tjs1\t0.03467610615829662\tjs2\t0.08861229001943892\ttrain disc cost\t-2.4008874893188477\ttime\t1.2681543413798015\n",
      "    5% Done! [Iteration]:       1500 [300x iterations time   ]:     395.82 secs [SUM]:    1976.63 secs\n",
      "iter 1499\tjs3\t0.1681332636472629\tjs4\t0.26091730296833393\tjs1\t0.023898865132894315\tjs2\t0.0715532144041106\ttrain disc cost\t-2.3251681327819824\ttime\t1.26819642384847\n",
      "    6% Done! [Iteration]:       1800 [300x iterations time   ]:     395.61 secs [SUM]:    2372.24 secs\n",
      "iter 1799\tjs3\t0.16588981101532904\tjs4\t0.2690588131512553\tjs1\t0.026074291712419814\tjs2\t0.07280238757872552\ttrain disc cost\t-2.3192195892333984\ttime\t1.2679399267832439\n",
      "    7% Done! [Iteration]:       2100 [300x iterations time   ]:     395.64 secs [SUM]:    2767.88 secs\n",
      "iter 2099\tjs3\t0.15907900859410504\tjs4\t0.26078166385005824\tjs1\t0.021044306172551774\tjs2\t0.06358295147445363\ttrain disc cost\t-2.3158743381500244\ttime\t1.2671675475438435\n",
      "    8% Done! [Iteration]:       2400 [300x iterations time   ]:     395.81 secs [SUM]:    3163.69 secs\n",
      "iter 2399\tjs3\t0.13818120315031607\tjs4\t0.2431665750763267\tjs1\t0.015875921638468425\tjs2\t0.0503087642287452\ttrain disc cost\t-2.2800517082214355\ttime\t1.2682714470227558\n",
      "    9% Done! [Iteration]:       2700 [300x iterations time   ]:     395.59 secs [SUM]:    3559.29 secs\n",
      "iter 2699\tjs3\t0.1528661283180588\tjs4\t0.25765821250426685\tjs1\t0.019133291009505633\tjs2\t0.05910667889049581\ttrain disc cost\t-2.2361719608306885\ttime\t1.2678404100735983\n",
      "   10% Done! [Iteration]:       3000 [300x iterations time   ]:     395.85 secs [SUM]:    3955.14 secs\n",
      "iter 2999\tjs3\t0.14435759856677677\tjs4\t0.24463349407387217\tjs1\t0.016946907630743847\tjs2\t0.05388673979905266\ttrain disc cost\t-2.229952096939087\ttime\t1.2687591004371643\n",
      "   11% Done! [Iteration]:       3300 [300x iterations time   ]:     395.85 secs [SUM]:    4350.99 secs\n",
      "iter 3299\tjs3\t0.14928412178626488\tjs4\t0.26164294953172795\tjs1\t0.01267341128057982\tjs2\t0.05019309303943444\ttrain disc cost\t-2.166119337081909\ttime\t1.2690518148740133\n",
      "   12% Done! [Iteration]:       3600 [300x iterations time   ]:     396.01 secs [SUM]:    4746.99 secs\n",
      "iter 3599\tjs3\t0.13968627540187212\tjs4\t0.2574284072071595\tjs1\t0.010685573112710338\tjs2\t0.04157983404605678\ttrain disc cost\t-2.180403232574463\ttime\t1.2688719646135966\n",
      "   13% Done! [Iteration]:       3900 [300x iterations time   ]:     396.07 secs [SUM]:    5143.06 secs\n",
      "iter 3899\tjs3\t0.1386243886573782\tjs4\t0.25499674828049884\tjs1\t0.010618126817282924\tjs2\t0.042363760994122264\ttrain disc cost\t-2.1729989051818848\ttime\t1.2689512475331624\n",
      "   14% Done! [Iteration]:       4200 [300x iterations time   ]:     395.99 secs [SUM]:    5539.05 secs\n",
      "iter 4199\tjs3\t0.12937124316388007\tjs4\t0.2362847347765848\tjs1\t0.009900040811965216\tjs2\t0.038404313880588375\ttrain disc cost\t-2.1622674465179443\ttime\t1.2690707278251647\n",
      "   15% Done! [Iteration]:       4500 [300x iterations time   ]:     395.87 secs [SUM]:    5934.92 secs\n",
      "iter 4499\tjs3\t0.12822932544700782\tjs4\t0.23389190016148892\tjs1\t0.010412353984143374\tjs2\t0.03772855023339088\ttrain disc cost\t-2.1977379322052\ttime\t1.2687888741493225\n",
      "   16% Done! [Iteration]:       4800 [300x iterations time   ]:     395.65 secs [SUM]:    6330.57 secs\n",
      "iter 4799\tjs3\t0.140547481498929\tjs4\t0.25261448414238696\tjs1\t0.01279357541519412\tjs2\t0.0452046006037889\ttrain disc cost\t-2.210005283355713\ttime\t1.2685703349113464\n",
      "   17% Done! [Iteration]:       5100 [300x iterations time   ]:     395.64 secs [SUM]:    6726.21 secs\n",
      "iter 5099\tjs3\t0.1284617326442819\tjs4\t0.24062569132132913\tjs1\t0.009873133282655829\tjs2\t0.03759680227903211\ttrain disc cost\t-2.1765670776367188\ttime\t1.2684420092900595\n",
      "   18% Done! [Iteration]:       5400 [300x iterations time   ]:     395.63 secs [SUM]:    7121.84 secs\n",
      "iter 5399\tjs3\t0.12584667464132107\tjs4\t0.23275777474294793\tjs1\t0.009113164958847626\tjs2\t0.03454329115445425\ttrain disc cost\t-2.2032718658447266\ttime\t1.2682569456100463\n",
      "   19% Done! [Iteration]:       5700 [300x iterations time   ]:     395.73 secs [SUM]:    7517.57 secs\n",
      "iter 5699\tjs3\t0.12808994726463901\tjs4\t0.2398360738563052\tjs1\t0.008602526104737315\tjs2\t0.03552593368201938\ttrain disc cost\t-2.2143802642822266\ttime\t1.2683724117279054\n",
      "   20% Done! [Iteration]:       6000 [300x iterations time   ]:     394.95 secs [SUM]:    7912.52 secs\n",
      "iter 5999\tjs3\t0.12666326491497\tjs4\t0.24033428854463976\tjs1\t0.00839982128761574\tjs2\t0.03509705868155952\ttrain disc cost\t-2.213437795639038\ttime\t1.2635025842984517\n",
      "   21% Done! [Iteration]:       6300 [300x iterations time   ]:     394.72 secs [SUM]:    8307.24 secs\n",
      "iter 6299\tjs3\t0.13120615898850055\tjs4\t0.24634325540849095\tjs1\t0.009330055649207341\tjs2\t0.03948670566836708\ttrain disc cost\t-2.249208450317383\ttime\t1.2640804529190064\n",
      "   22% Done! [Iteration]:       6600 [300x iterations time   ]:     395.18 secs [SUM]:    8702.42 secs\n",
      "iter 6599\tjs3\t0.12883291797448612\tjs4\t0.23971443848723695\tjs1\t0.009249515041085274\tjs2\t0.036480002726648794\ttrain disc cost\t-2.2083940505981445\ttime\t1.2645346291859945\n",
      "   23% Done! [Iteration]:       6900 [300x iterations time   ]:     394.96 secs [SUM]:    9097.38 secs\n",
      "iter 6899\tjs3\t0.12122396486202937\tjs4\t0.2333297183556123\tjs1\t0.007511964305572693\tjs2\t0.030863591497515008\ttrain disc cost\t-2.1867361068725586\ttime\t1.2648164677619933\n",
      "   24% Done! [Iteration]:       7200 [300x iterations time   ]:     394.95 secs [SUM]:    9492.33 secs\n",
      "iter 7199\tjs3\t0.12268337108276789\tjs4\t0.24258272931268646\tjs1\t0.007552103067722465\tjs2\t0.03206137179475237\ttrain disc cost\t-2.178621768951416\ttime\t1.2643358739217123\n",
      "   25% Done! [Iteration]:       7500 [300x iterations time   ]:     394.98 secs [SUM]:    9887.31 secs\n",
      "iter 7499\tjs3\t0.12996503856208338\tjs4\t0.2487018527282624\tjs1\t0.009946438372499606\tjs2\t0.037520074292356834\ttrain disc cost\t-2.2238082885742188\ttime\t1.2643197019894918\n",
      "   26% Done! [Iteration]:       7800 [300x iterations time   ]:     395.00 secs [SUM]:   10282.31 secs\n",
      "iter 7799\tjs3\t0.12099749522615785\tjs4\t0.22715678347505172\tjs1\t0.008180704112895353\tjs2\t0.03300941855743644\ttrain disc cost\t-2.264033317565918\ttime\t1.2643445833524067\n",
      "   27% Done! [Iteration]:       8100 [300x iterations time   ]:     394.95 secs [SUM]:   10677.26 secs\n",
      "iter 8099\tjs3\t0.1261940686846575\tjs4\t0.23285614294471643\tjs1\t0.010746920341701565\tjs2\t0.03660527322273655\ttrain disc cost\t-2.2385103702545166\ttime\t1.2643137566248577\n",
      "   28% Done! [Iteration]:       8400 [300x iterations time   ]:     394.93 secs [SUM]:   11072.18 secs\n",
      "iter 8399\tjs3\t0.11743594205248291\tjs4\t0.23368882270970107\tjs1\t0.003975492396317325\tjs2\t0.026194975750925987\ttrain disc cost\t-2.1641643047332764\ttime\t1.2642539525032044\n",
      "   29% Done! [Iteration]:       8700 [300x iterations time   ]:     394.81 secs [SUM]:   11466.99 secs\n",
      "iter 8699\tjs3\t0.11309521523768937\tjs4\t0.22489696441178098\tjs1\t0.0051876942951915245\tjs2\t0.026609279917269257\ttrain disc cost\t-2.2180709838867188\ttime\t1.263889832496643\n",
      "   30% Done! [Iteration]:       9000 [300x iterations time   ]:     394.79 secs [SUM]:   11861.78 secs\n",
      "iter 8999\tjs3\t0.12870783177223016\tjs4\t0.25476718975128165\tjs1\t0.0056020892402124\tjs2\t0.029595308148143838\ttrain disc cost\t-2.207507848739624\ttime\t1.2639293471972148\n",
      "   31% Done! [Iteration]:       9300 [300x iterations time   ]:     395.00 secs [SUM]:   12256.78 secs\n",
      "iter 9299\tjs3\t0.12175235437356066\tjs4\t0.24551164605806755\tjs1\t0.004727303379813012\tjs2\t0.02755813511525627\ttrain disc cost\t-2.159557342529297\ttime\t1.2643720650672912\n",
      "   32% Done! [Iteration]:       9600 [300x iterations time   ]:     394.89 secs [SUM]:   12651.67 secs\n",
      "iter 9599\tjs3\t0.11752378535000597\tjs4\t0.23640667358511933\tjs1\t0.0035672729588652613\tjs2\t0.024718240948461784\ttrain disc cost\t-2.1453495025634766\ttime\t1.2640581552187602\n",
      "   33% Done! [Iteration]:       9900 [300x iterations time   ]:     394.64 secs [SUM]:   13046.30 secs\n",
      "iter 9899\tjs3\t0.11476987145361123\tjs4\t0.22792796848684294\tjs1\t0.005273922578728901\tjs2\t0.025792771543568464\ttrain disc cost\t-2.156707286834717\ttime\t1.2630564761161804\n",
      "   34% Done! [Iteration]:      10200 [300x iterations time   ]:     394.99 secs [SUM]:   13441.30 secs\n",
      "iter 10199\tjs3\t0.11809136230205025\tjs4\t0.23350329734170006\tjs1\t0.005597730674514379\tjs2\t0.02645248116599622\ttrain disc cost\t-2.1740574836730957\ttime\t1.2642306868235271\n",
      "   35% Done! [Iteration]:      10500 [300x iterations time   ]:     395.15 secs [SUM]:   13836.45 secs\n",
      "iter 10499\tjs3\t0.11731215332977836\tjs4\t0.23630995203503705\tjs1\t0.00421321407809115\tjs2\t0.025735625659499675\ttrain disc cost\t-2.229570150375366\ttime\t1.2642121505737305\n",
      "   36% Done! [Iteration]:      10800 [300x iterations time   ]:     395.11 secs [SUM]:   14231.56 secs\n",
      "iter 10799\tjs3\t0.11974370537815122\tjs4\t0.24443657018924117\tjs1\t0.00407519372612718\tjs2\t0.025634902727770956\ttrain disc cost\t-2.2098629474639893\ttime\t1.2642217214902243\n",
      "   37% Done! [Iteration]:      11100 [300x iterations time   ]:     395.00 secs [SUM]:   14626.56 secs\n",
      "iter 11099\tjs3\t0.11311194031251808\tjs4\t0.2182238472772103\tjs1\t0.005453182700256047\tjs2\t0.027181204371045646\ttrain disc cost\t-2.1907799243927\ttime\t1.2640461818377178\n",
      "   38% Done! [Iteration]:      11400 [300x iterations time   ]:     394.97 secs [SUM]:   15021.53 secs\n",
      "iter 11399\tjs3\t0.11653083357549737\tjs4\t0.23211242216854958\tjs1\t0.003812997764530767\tjs2\t0.0242319629297138\ttrain disc cost\t-2.258594512939453\ttime\t1.2638347260157268\n",
      "   39% Done! [Iteration]:      11700 [300x iterations time   ]:     394.80 secs [SUM]:   15416.34 secs\n",
      "iter 11699\tjs3\t0.11363188713329031\tjs4\t0.2245752166329982\tjs1\t0.004170051269221708\tjs2\t0.02449144736678838\ttrain disc cost\t-2.2016706466674805\ttime\t1.2640992267926534\n",
      "   40% Done! [Iteration]:      12000 [300x iterations time   ]:     394.91 secs [SUM]:   15811.25 secs\n",
      "iter 11999\tjs3\t0.11795041322431594\tjs4\t0.23176188100167988\tjs1\t0.0046484767115624204\tjs2\t0.025621029766367376\ttrain disc cost\t-2.2164981365203857\ttime\t1.2639502437909444\n",
      "   41% Done! [Iteration]:      12300 [300x iterations time   ]:     395.03 secs [SUM]:   16206.28 secs\n",
      "iter 12299\tjs3\t0.11613326459160829\tjs4\t0.23403212213853106\tjs1\t0.0022959750583329084\tjs2\t0.022192984482736496\ttrain disc cost\t-2.14074444770813\ttime\t1.2640777373313903\n",
      "   42% Done! [Iteration]:      12600 [300x iterations time   ]:     394.93 secs [SUM]:   16601.21 secs\n",
      "iter 12599\tjs3\t0.1117546654426526\tjs4\t0.22713869968326136\tjs1\t0.002861275279593838\tjs2\t0.022596871976724796\ttrain disc cost\t-2.1377811431884766\ttime\t1.2640110381444296\n",
      "   43% Done! [Iteration]:      12900 [300x iterations time   ]:     394.54 secs [SUM]:   16995.75 secs\n",
      "iter 12899\tjs3\t0.12339803300895341\tjs4\t0.23785900949247077\tjs1\t0.0033428009727819066\tjs2\t0.02812617186978848\ttrain disc cost\t-2.1577134132385254\ttime\t1.264157481988271\n",
      "   44% Done! [Iteration]:      13200 [300x iterations time   ]:     394.74 secs [SUM]:   17390.49 secs\n",
      "iter 13199\tjs3\t0.11381154168177308\tjs4\t0.2254611734272004\tjs1\t0.002907291764957675\tjs2\t0.023316634886620963\ttrain disc cost\t-2.1323771476745605\ttime\t1.2638700620333354\n",
      "   45% Done! [Iteration]:      13500 [300x iterations time   ]:     394.71 secs [SUM]:   17785.20 secs\n",
      "iter 13499\tjs3\t0.11647517982064368\tjs4\t0.22034391079115212\tjs1\t0.005693137442336864\tjs2\t0.027336446831556022\ttrain disc cost\t-2.2130160331726074\ttime\t1.263745042483012\n",
      "   46% Done! [Iteration]:      13800 [300x iterations time   ]:     394.86 secs [SUM]:   18180.06 secs\n",
      "iter 13799\tjs3\t0.12255537294647077\tjs4\t0.24250345892617822\tjs1\t0.003998601486554028\tjs2\t0.029101131302116546\ttrain disc cost\t-2.1857306957244873\ttime\t1.2638115612665812\n",
      "   47% Done! [Iteration]:      14100 [300x iterations time   ]:     394.91 secs [SUM]:   18574.97 secs\n",
      "iter 14099\tjs3\t0.11730620258796715\tjs4\t0.23240803327532808\tjs1\t0.0022603476409780027\tjs2\t0.0233999688540005\ttrain disc cost\t-2.1916019916534424\ttime\t1.26393257300059\n",
      "   48% Done! [Iteration]:      14400 [300x iterations time   ]:     394.82 secs [SUM]:   18969.79 secs\n",
      "iter 14399\tjs3\t0.11560157839524327\tjs4\t0.23097627638546084\tjs1\t0.0028703436026875514\tjs2\t0.0237117575234763\ttrain disc cost\t-2.2138235569000244\ttime\t1.2639167173703512\n",
      "   49% Done! [Iteration]:      14700 [300x iterations time   ]:     394.69 secs [SUM]:   19364.48 secs\n",
      "iter 14699\tjs3\t0.11668758543147936\tjs4\t0.23810402878891326\tjs1\t0.0030494620515476847\tjs2\t0.023759346556240256\ttrain disc cost\t-2.211742639541626\ttime\t1.2636747169494629\n",
      "   50% Done! [Iteration]:      15000 [300x iterations time   ]:     394.89 secs [SUM]:   19759.37 secs\n",
      "iter 14999\tjs3\t0.11669014537308757\tjs4\t0.24051116233858702\tjs1\t0.0019833667845331605\tjs2\t0.022809146974443438\ttrain disc cost\t-2.1776442527770996\ttime\t1.2642181380589803\n",
      "   51% Done! [Iteration]:      15300 [300x iterations time   ]:     394.84 secs [SUM]:   20154.21 secs\n",
      "iter 15299\tjs3\t0.11417876300368603\tjs4\t0.22895009631830376\tjs1\t0.0018258106395167178\tjs2\t0.024666959970150993\ttrain disc cost\t-2.1926655769348145\ttime\t1.2640252494812012\n",
      "   52% Done! [Iteration]:      15600 [300x iterations time   ]:     394.89 secs [SUM]:   20549.09 secs\n",
      "iter 15599\tjs3\t0.1172862096144867\tjs4\t0.2271483403679184\tjs1\t0.004434992562326756\tjs2\t0.027002649950068777\ttrain disc cost\t-2.2463061809539795\ttime\t1.2638421217600504\n",
      "   53% Done! [Iteration]:      15900 [300x iterations time   ]:     394.70 secs [SUM]:   20943.79 secs\n",
      "iter 15899\tjs3\t0.11470327195740618\tjs4\t0.23091064348917797\tjs1\t0.0016737372591395896\tjs2\t0.02268865513427348\ttrain disc cost\t-2.2029738426208496\ttime\t1.2633102822303772\n",
      "   54% Done! [Iteration]:      16200 [300x iterations time   ]:     394.77 secs [SUM]:   21338.56 secs\n",
      "iter 16199\tjs3\t0.11351492582636295\tjs4\t0.2235661735893776\tjs1\t0.0022212050414442677\tjs2\t0.02442236660827975\ttrain disc cost\t-2.242327928543091\ttime\t1.2635649506251017\n",
      "   55% Done! [Iteration]:      16500 [300x iterations time   ]:     394.61 secs [SUM]:   21733.17 secs\n",
      "iter 16499\tjs3\t0.11755059494928817\tjs4\t0.2352070442504889\tjs1\t0.0019501629846222928\tjs2\t0.023388054903923296\ttrain disc cost\t-2.231886863708496\ttime\t1.2637866560618083\n",
      "   56% Done! [Iteration]:      16800 [300x iterations time   ]:     394.79 secs [SUM]:   22127.96 secs\n",
      "iter 16799\tjs3\t0.11732767082765812\tjs4\t0.23363956558606147\tjs1\t0.003099995764618956\tjs2\t0.02334610336597841\ttrain disc cost\t-2.3485608100891113\ttime\t1.2634864242871602\n",
      "   57% Done! [Iteration]:      17100 [300x iterations time   ]:     394.66 secs [SUM]:   22522.62 secs\n",
      "iter 17099\tjs3\t0.11262670457031493\tjs4\t0.22751495875743716\tjs1\t0.002320116228768395\tjs2\t0.0240591250748103\ttrain disc cost\t-2.3237478733062744\ttime\t1.263046421209971\n",
      "   58% Done! [Iteration]:      17400 [300x iterations time   ]:     394.79 secs [SUM]:   22917.41 secs\n",
      "iter 17399\tjs3\t0.11696648991797719\tjs4\t0.23353525597049263\tjs1\t0.0019707885270777237\tjs2\t0.02359091738125714\ttrain disc cost\t-2.2254340648651123\ttime\t1.2633037416140238\n",
      "   59% Done! [Iteration]:      17700 [300x iterations time   ]:     394.74 secs [SUM]:   23312.15 secs\n",
      "iter 17699\tjs3\t0.11409878508067756\tjs4\t0.229890554245122\tjs1\t0.0024572028923727017\tjs2\t0.023579145138204718\ttrain disc cost\t-2.285801887512207\ttime\t1.2632775656382242\n",
      "   60% Done! [Iteration]:      18000 [300x iterations time   ]:     394.75 secs [SUM]:   23706.90 secs\n",
      "iter 17999\tjs3\t0.1148056794087315\tjs4\t0.22579020311166928\tjs1\t0.002940662930953891\tjs2\t0.025636514252132772\ttrain disc cost\t-2.2308804988861084\ttime\t1.2633840409914652\n",
      "   61% Done! [Iteration]:      18300 [300x iterations time   ]:     394.56 secs [SUM]:   24101.46 secs\n",
      "iter 18299\tjs3\t0.11149471885292418\tjs4\t0.2139338476686587\tjs1\t0.003731065488886163\tjs2\t0.02458214323926006\ttrain disc cost\t-2.2319233417510986\ttime\t1.263198703924815\n",
      "   62% Done! [Iteration]:      18600 [300x iterations time   ]:     394.71 secs [SUM]:   24496.17 secs\n",
      "iter 18599\tjs3\t0.11168806388096679\tjs4\t0.22308972232208754\tjs1\t0.002085851541434195\tjs2\t0.022247367736789418\ttrain disc cost\t-2.2915163040161133\ttime\t1.2633547727266947\n",
      "   63% Done! [Iteration]:      18900 [300x iterations time   ]:     394.76 secs [SUM]:   24890.93 secs\n",
      "iter 18899\tjs3\t0.12217525116477676\tjs4\t0.23589552300023256\tjs1\t0.0027840067446474417\tjs2\t0.02689743334010022\ttrain disc cost\t-2.2373673915863037\ttime\t1.2633682322502136\n",
      "   64% Done! [Iteration]:      19200 [300x iterations time   ]:     394.63 secs [SUM]:   25285.56 secs\n",
      "iter 19199\tjs3\t0.1153440653870324\tjs4\t0.22690620987179194\tjs1\t0.0025249097337644356\tjs2\t0.02516751285391374\ttrain disc cost\t-2.2398126125335693\ttime\t1.26331955909729\n",
      "   65% Done! [Iteration]:      19500 [300x iterations time   ]:     394.81 secs [SUM]:   25680.37 secs\n",
      "iter 19499\tjs3\t0.10981944661289018\tjs4\t0.23467644816581923\tjs1\t0.0020226821459160098\tjs2\t0.020437032679829128\ttrain disc cost\t-2.290846109390259\ttime\t1.2632755422592163\n",
      "   66% Done! [Iteration]:      19800 [300x iterations time   ]:     394.80 secs [SUM]:   26075.17 secs\n",
      "iter 19799\tjs3\t0.11463096507570067\tjs4\t0.22701345120181518\tjs1\t0.0030154361227190335\tjs2\t0.025176276436088312\ttrain disc cost\t-2.2172553539276123\ttime\t1.263451714515686\n",
      "   67% Done! [Iteration]:      20100 [300x iterations time   ]:     394.61 secs [SUM]:   26469.78 secs\n",
      "iter 20099\tjs3\t0.1142711674461501\tjs4\t0.2192732274125378\tjs1\t0.004375622033578099\tjs2\t0.024961228366871428\ttrain disc cost\t-2.220022678375244\ttime\t1.2630274033546447\n",
      "   68% Done! [Iteration]:      20400 [300x iterations time   ]:     394.81 secs [SUM]:   26864.59 secs\n",
      "iter 20399\tjs3\t0.11289617749776922\tjs4\t0.2349713995535297\tjs1\t0.0019239051775658351\tjs2\t0.02227418937734047\ttrain disc cost\t-2.3186888694763184\ttime\t1.2636490535736085\n",
      "   69% Done! [Iteration]:      20700 [300x iterations time   ]:     394.65 secs [SUM]:   27259.24 secs\n",
      "iter 20699\tjs3\t0.11397065805669586\tjs4\t0.21812912703881326\tjs1\t0.0042895324969614\tjs2\t0.024664455646976776\ttrain disc cost\t-2.2404744625091553\ttime\t1.2632744352022807\n",
      "   70% Done! [Iteration]:      21000 [300x iterations time   ]:     394.80 secs [SUM]:   27654.04 secs\n",
      "iter 20999\tjs3\t0.11624171940292227\tjs4\t0.23413327018618385\tjs1\t0.0018503245721501963\tjs2\t0.022992918122016932\ttrain disc cost\t-2.236961841583252\ttime\t1.263215075333913\n",
      "   71% Done! [Iteration]:      21300 [300x iterations time   ]:     394.84 secs [SUM]:   28048.88 secs\n",
      "iter 21299\tjs3\t0.11451074783550035\tjs4\t0.22474300360618726\tjs1\t0.003415124993220308\tjs2\t0.024425572176074044\ttrain disc cost\t-2.2565369606018066\ttime\t1.2634059500694275\n",
      "   72% Done! [Iteration]:      21600 [300x iterations time   ]:     394.80 secs [SUM]:   28443.68 secs\n",
      "iter 21599\tjs3\t0.12438542833059255\tjs4\t0.2209389640776862\tjs1\t0.007395211038058035\tjs2\t0.03168571290843485\ttrain disc cost\t-2.2324447631835938\ttime\t1.263539887269338\n",
      "   73% Done! [Iteration]:      21900 [300x iterations time   ]:     394.69 secs [SUM]:   28838.37 secs\n",
      "iter 21899\tjs3\t0.12164729171602558\tjs4\t0.24199831053233722\tjs1\t0.003948822582207265\tjs2\t0.026989679880796764\ttrain disc cost\t-2.3911004066467285\ttime\t1.2632831978797912\n",
      "   74% Done! [Iteration]:      22200 [300x iterations time   ]:     394.81 secs [SUM]:   29233.18 secs\n",
      "iter 22199\tjs3\t0.112368224511174\tjs4\t0.22804293186147986\tjs1\t0.002490626041499855\tjs2\t0.02345321204229588\ttrain disc cost\t-2.306978225708008\ttime\t1.2635483932495117\n",
      "   75% Done! [Iteration]:      22500 [300x iterations time   ]:     394.77 secs [SUM]:   29627.95 secs\n",
      "iter 22499\tjs3\t0.11559400126736621\tjs4\t0.233660018410291\tjs1\t0.0027033266226644736\tjs2\t0.0247427498549769\ttrain disc cost\t-2.288351535797119\ttime\t1.2633426133791605\n",
      "   76% Done! [Iteration]:      22800 [300x iterations time   ]:     394.75 secs [SUM]:   30022.69 secs\n",
      "iter 22799\tjs3\t0.11455039121562269\tjs4\t0.23606005295892593\tjs1\t0.0017221889799331448\tjs2\t0.022009945261996153\ttrain disc cost\t-2.282285690307617\ttime\t1.2632781966527302\n",
      "   77% Done! [Iteration]:      23100 [300x iterations time   ]:     395.04 secs [SUM]:   30417.73 secs\n",
      "iter 23099\tjs3\t0.11003903387252616\tjs4\t0.22284084233500348\tjs1\t0.0019079798232114534\tjs2\t0.02145084207466626\ttrain disc cost\t-2.3780930042266846\ttime\t1.2637067111333211\n",
      "   78% Done! [Iteration]:      23400 [300x iterations time   ]:     394.79 secs [SUM]:   30812.52 secs\n",
      "iter 23399\tjs3\t0.11136967228578572\tjs4\t0.22743395646986683\tjs1\t0.0016246893209504974\tjs2\t0.02216317768626381\ttrain disc cost\t-2.276197910308838\ttime\t1.2636461965243022\n",
      "   79% Done! [Iteration]:      23700 [300x iterations time   ]:     394.75 secs [SUM]:   31207.27 secs\n",
      "iter 23699\tjs3\t0.11565635900766544\tjs4\t0.22051734372193058\tjs1\t0.0031886471303361093\tjs2\t0.0253390724791623\ttrain disc cost\t-2.2785685062408447\ttime\t1.2634458057085673\n",
      "   80% Done! [Iteration]:      24000 [300x iterations time   ]:     395.17 secs [SUM]:   31602.44 secs\n",
      "iter 23999\tjs3\t0.11535241735318301\tjs4\t0.2281169003311468\tjs1\t0.0030507605001778076\tjs2\t0.025080303765097983\ttrain disc cost\t-2.2811105251312256\ttime\t1.2635703539848329\n",
      "   81% Done! [Iteration]:      24300 [300x iterations time   ]:     394.87 secs [SUM]:   31997.30 secs\n",
      "iter 24299\tjs3\t0.11523082614660288\tjs4\t0.21099140421274748\tjs1\t0.006414211461568499\tjs2\t0.02906926771709419\ttrain disc cost\t-2.2616846561431885\ttime\t1.263354655901591\n",
      "   82% Done! [Iteration]:      24600 [300x iterations time   ]:     394.99 secs [SUM]:   32392.29 secs\n",
      "iter 24599\tjs3\t0.11030245723035996\tjs4\t0.22133890509555143\tjs1\t0.0023929403795137547\tjs2\t0.023056400557210484\ttrain disc cost\t-2.3752810955047607\ttime\t1.263438180287679\n",
      "   83% Done! [Iteration]:      24900 [300x iterations time   ]:     394.97 secs [SUM]:   32787.25 secs\n",
      "iter 24899\tjs3\t0.116112313901525\tjs4\t0.23499480492377275\tjs1\t0.0027585384489102792\tjs2\t0.023319513064524424\ttrain disc cost\t-2.218478202819824\ttime\t1.2633684627215067\n",
      "   84% Done! [Iteration]:      25200 [300x iterations time   ]:     394.98 secs [SUM]:   33182.24 secs\n",
      "iter 25199\tjs3\t0.11008458208338964\tjs4\t0.22665068390055393\tjs1\t0.001827105499312061\tjs2\t0.022804041438558232\ttrain disc cost\t-2.2208943367004395\ttime\t1.2634050305684408\n",
      "   85% Done! [Iteration]:      25500 [300x iterations time   ]:     395.15 secs [SUM]:   33577.38 secs\n",
      "iter 25499\tjs3\t0.11587078419325178\tjs4\t0.23573667081282165\tjs1\t0.001953658163160505\tjs2\t0.022785133155163118\ttrain disc cost\t-2.2446367740631104\ttime\t1.264039586385091\n",
      "   86% Done! [Iteration]:      25800 [300x iterations time   ]:     395.22 secs [SUM]:   33972.60 secs\n",
      "iter 25799\tjs3\t0.11174433121346534\tjs4\t0.2285296848335904\tjs1\t0.0016594709955455535\tjs2\t0.021183223817036773\ttrain disc cost\t-2.268653154373169\ttime\t1.2639108538627624\n",
      "   87% Done! [Iteration]:      26100 [300x iterations time   ]:     395.03 secs [SUM]:   34367.63 secs\n",
      "iter 26099\tjs3\t0.11403766198009041\tjs4\t0.23117947981302064\tjs1\t0.001581508692751197\tjs2\t0.02320166463233292\ttrain disc cost\t-2.1953749656677246\ttime\t1.2635340468088785\n",
      "   88% Done! [Iteration]:      26400 [300x iterations time   ]:     394.87 secs [SUM]:   34762.50 secs\n",
      "iter 26399\tjs3\t0.1134366292334278\tjs4\t0.21373007284189335\tjs1\t0.004421289251014002\tjs2\t0.025749963739408126\ttrain disc cost\t-2.277385950088501\ttime\t1.263467116355896\n",
      "   89% Done! [Iteration]:      26700 [300x iterations time   ]:     394.86 secs [SUM]:   35157.36 secs\n",
      "iter 26699\tjs3\t0.1124097046859703\tjs4\t0.22068132280311306\tjs1\t0.0025424572390216416\tjs2\t0.025164783423309217\ttrain disc cost\t-2.247776985168457\ttime\t1.263430347442627\n",
      "   90% Done! [Iteration]:      27000 [300x iterations time   ]:     394.83 secs [SUM]:   35552.20 secs\n",
      "iter 26999\tjs3\t0.11807085349503701\tjs4\t0.23183652024087556\tjs1\t0.0024320910785006634\tjs2\t0.024581208262276086\ttrain disc cost\t-2.2722389698028564\ttime\t1.2636795059839885\n",
      "   91% Done! [Iteration]:      27300 [300x iterations time   ]:     394.94 secs [SUM]:   35947.13 secs\n",
      "iter 27299\tjs3\t0.11365257473937718\tjs4\t0.22285858427021\tjs1\t0.0030384778589243365\tjs2\t0.02453827252520794\ttrain disc cost\t-2.256181478500366\ttime\t1.2637036689122518\n",
      "   92% Done! [Iteration]:      27600 [300x iterations time   ]:     394.89 secs [SUM]:   36342.03 secs\n",
      "iter 27599\tjs3\t0.11347713228753359\tjs4\t0.22369331535630357\tjs1\t0.002173424102909069\tjs2\t0.022890628664688613\ttrain disc cost\t-2.245933771133423\ttime\t1.263374695777893\n",
      "   93% Done! [Iteration]:      27900 [300x iterations time   ]:     394.77 secs [SUM]:   36736.79 secs\n",
      "iter 27899\tjs3\t0.1114877052670023\tjs4\t0.22474323519022374\tjs1\t0.0021484021978045503\tjs2\t0.022890876951639553\ttrain disc cost\t-2.240628957748413\ttime\t1.2631792640686035\n",
      "   94% Done! [Iteration]:      28200 [300x iterations time   ]:     394.98 secs [SUM]:   37131.77 secs\n",
      "iter 28199\tjs3\t0.11366638953252521\tjs4\t0.21495245861090975\tjs1\t0.004896521695260019\tjs2\t0.02618756200486562\ttrain disc cost\t-2.258392810821533\ttime\t1.263254260222117\n",
      "   95% Done! [Iteration]:      28500 [300x iterations time   ]:     394.70 secs [SUM]:   37526.47 secs\n",
      "iter 28499\tjs3\t0.11403478351190383\tjs4\t0.22542377145643513\tjs1\t0.0022476017358879826\tjs2\t0.022938467831652598\ttrain disc cost\t-2.279245615005493\ttime\t1.263120265007019\n",
      "   96% Done! [Iteration]:      28800 [300x iterations time   ]:     394.86 secs [SUM]:   37921.33 secs\n",
      "iter 28799\tjs3\t0.11256799214269306\tjs4\t0.23646665562338665\tjs1\t0.0032749673672809753\tjs2\t0.023590255099075384\ttrain disc cost\t-2.2952888011932373\ttime\t1.2636005902290344\n",
      "   97% Done! [Iteration]:      29100 [300x iterations time   ]:     395.06 secs [SUM]:   38316.39 secs\n",
      "iter 29099\tjs3\t0.11693954742731219\tjs4\t0.22269995957539876\tjs1\t0.0030324787103148273\tjs2\t0.02427216827179871\ttrain disc cost\t-2.3078372478485107\ttime\t1.2636187847455342\n",
      "   98% Done! [Iteration]:      29400 [300x iterations time   ]:     395.02 secs [SUM]:   38711.41 secs\n",
      "iter 29399\tjs3\t0.11089777361592462\tjs4\t0.21081643041025155\tjs1\t0.00507382404546336\tjs2\t0.025969013465915656\ttrain disc cost\t-2.2690236568450928\ttime\t1.2635672934850057\n",
      "   99% Done! [Iteration]:      29700 [300x iterations time   ]:     394.74 secs [SUM]:   39106.15 secs\n",
      "iter 29699\tjs3\t0.10977655314417227\tjs4\t0.2274917070662377\tjs1\t0.001819095623633171\tjs2\t0.02049865962551725\ttrain disc cost\t-2.2665138244628906\ttime\t1.2631674567858378\n",
      "  100% Done! [Iteration]:      30000 [300x iterations time   ]:     394.85 secs [SUM]:   39501.00 secs\n",
      "iter 29999\tjs3\t0.10917434669822584\tjs4\t0.22077255759126707\tjs1\t0.0021554623439311096\tjs2\t0.02311024573113141\ttrain disc cost\t-2.2246146202087402\ttime\t1.2633504605293273\n"
     ]
    }
   ],
   "source": [
    "sum_disc_cost = []\n",
    "js1 =[]\n",
    "js2 =[]\n",
    "js3 =[]\n",
    "js4 =[]\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    #初始化模型的参数\n",
    "    session.run(tf.global_variables_initializer())\n",
    "   \n",
    "    def generate_samples():\n",
    "        samples = session.run(fake_inputs)\n",
    "        #对于三维度矩阵，a有三个方向a[0][1][2]，按照a[2]方向找最大值，返回最大值索引值组成矩阵\n",
    "        samples = np.argmax(samples, axis=2)\n",
    "        decoded_samples = []\n",
    "        for i in range(len(samples)):\n",
    "            decoded = []\n",
    "            for j in range(len(samples[i])):\n",
    "                decoded.append(inv_charmap[samples[i][j]])\n",
    "            decoded_samples.append(tuple(decoded))\n",
    "        return decoded_samples\n",
    "    # 获取数据，迭代一定的数据\n",
    "    gen = inf_train_gen()\n",
    "\n",
    "    sum_time = 0.\n",
    "    line_time = 0. \n",
    "    loading_str = \"*\"\n",
    "    # 训练3万轮\n",
    "    for iteration in range(ITERS):\n",
    "         # 记录当前时间\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if (iteration == 0):\n",
    "            now_time = time.clock()\n",
    "            print(\"[Start]\")\n",
    "\n",
    "        # Train generator（第二轮开始训练生成器）\n",
    "        if iteration > 0:\n",
    "            _ = session.run(gen_train_op)\n",
    "            # _gen_cost,_ = session.run(gen_cost,gen_train_op) 自己写的\n",
    "\n",
    "        # Train critic\n",
    "        for i in range(CRITIC_ITERS):\n",
    "            _data = gen.__next__() # 1个critic 就迭代 10 轮的数据\n",
    "            _disc_cost, _ = session.run(\n",
    "                [disc_cost, disc_train_op],\n",
    "                feed_dict={real_inputs_discrete:_data}\n",
    "            )\n",
    "            \n",
    "            #print(\"_disc_cost \"+str(_disc_cost))\n",
    "            #print(\"_ \"+str(_))\n",
    "            #print(\"_data \"+str(_data))\n",
    "            #print(\"gen_cost \"+str(gen_cost))\n",
    "            #print(\"disc_cost\"+str(disc_cost))\n",
    "\n",
    "        # How many iterations to change line \n",
    "        change_line=int(ITERS/1000)\n",
    "        \n",
    "        after_time=time.clock() - now_time\n",
    "        sum_time+=after_time\n",
    "         # 预计时间（以当前的单位迭代时间来预计运行完整个project要多长时间）\n",
    "        eta_time = (ITERS-iteration)*(after_time)\n",
    "        \n",
    "        # 单位迭代时间  \n",
    "        print(\"[{1:10}] [Iteration]: {0:10} [Unit iteration time    ]: {2:10.2f} secs [ETA]: {3:10.2f} secs\".format( (iteration+1), loading_str, after_time, eta_time) , end=\"\\r\")\n",
    "        now_time = time.clock()\n",
    "        # 如果等于 迭代数\n",
    "        if iteration % change_line == (change_line-1):\n",
    "            loading_str += \"*\"\n",
    "            if iteration % (10*change_line) == (10*change_line-1):\n",
    "                # 输出总时间 ，进度条的百分几\n",
    "                print(\"{5:5.0f}{0:7} [Iteration]: {1:10} [{2:23}]: {3:10.2f} secs [SUM]: {4:10.2f}\".format(\"% Done!\", (iteration+1), (str(10*change_line)+\"x iterations time\"), (sum_time-line_time), sum_time, (100*iteration/ITERS) ) )\n",
    "                loading_str = \"*\"\n",
    "                line_time = sum_time\n",
    "        # 输出每轮的时间到统计图\n",
    "        lib.plot.plot('time', time.time() - start_time)\n",
    "         # 输出每轮的_disc_cost到统计图\n",
    "        lib.plot.plot('train disc cost', _disc_cost)   \n",
    "        sum_disc_cost.append(_disc_cost) \n",
    "\n",
    "        if iteration % (10*change_line) == (10*change_line-1):\n",
    "            #print(\"checkpintB\"+str(iteration+1))\n",
    "            samples = []\n",
    "            for i in range(10):\n",
    "                samples.extend(generate_samples())\n",
    "            #js 离散度,越小越好https://cloud.tencent.com/developer/article/1530349\n",
    "            # for i in range(4):\n",
    "            #     lm = language_helpers.NgramLanguageModel(i+1, samples, tokenize=False)\n",
    "            #     lib.plot.plot('js{}'.format(i+1), lm.js_with(true_char_ngram_lms[i]))\n",
    "            \n",
    "            #自己写的：代替循环保存js数据\n",
    "            #range = 0\n",
    "            lm_0 = language_helpers.NgramLanguageModel(0+1, samples, tokenize=False)\n",
    "            lib.plot.plot('js{}'.format(0+1), lm_0.js_with(true_char_ngram_lms[0]))\n",
    "            js1.append(lm_0.js_with(true_char_ngram_lms[0]))\n",
    "            #range = 1\n",
    "            lm_1 = language_helpers.NgramLanguageModel(1+1, samples, tokenize=False)\n",
    "            lib.plot.plot('js{}'.format(1+1), lm_1.js_with(true_char_ngram_lms[1]))\n",
    "            js2.append(lm_1.js_with(true_char_ngram_lms[1]))\n",
    "            #range = 2\n",
    "            lm_2 = language_helpers.NgramLanguageModel(2+1, samples, tokenize=False)\n",
    "            lib.plot.plot('js{}'.format(2+1), lm_2.js_with(true_char_ngram_lms[2]))\n",
    "            js3.append( lm_2.js_with(true_char_ngram_lms[2]))\n",
    "            #range = 3\n",
    "            lm_3 = language_helpers.NgramLanguageModel(3+1, samples, tokenize=False)\n",
    "            lib.plot.plot('js{}'.format(3+1), lm_3.js_with(true_char_ngram_lms[3]))\n",
    "            js4.append( lm_3.js_with(true_char_ngram_lms[3])) \n",
    "            \n",
    "            \n",
    "            with open('output_data/samples_{}.txt'.format(str(iteration+1).zfill(7)), 'w',encoding = 'utf8') as f:\n",
    "                for s in samples:\n",
    "                    s = \"\".join(s)\n",
    "                    s = language_helpers.checkDNSFrom(s)\n",
    "                    f.write(str(s) + \"\\n\")\n",
    "\n",
    "        if iteration % (10*change_line) == (10*change_line-1):\n",
    "            #print(iteration)\n",
    "            lib.plot.flush()\n",
    "        \n",
    "        lib.plot.tick()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}