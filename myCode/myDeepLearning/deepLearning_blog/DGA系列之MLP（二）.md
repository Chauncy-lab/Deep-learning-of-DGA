继上一篇DGA深度学习方面的应用之后，我们来深扒一下MLP多层感知机的算法的原理和代码实践
## 发展史
针对机器学习对深度学习技术的发展过渡，发表一些个人的理解。一开始的机器学习，是基于特征去实现让机器具有识别工作，但需要识别的目标数量变得大的时候，机器学习所需要训练的数量也会变大，而这个人们初期用的方法是在同一个维度的横向扩展（不断地使用特征数据训练），这样的确可以达到高的命中率，构造一个完美的分类曲线。但是这个训练出来的模型没有泛化的能力，与数据过拟合了。举个例子：一个人面部特征，有千万个点组成，它可以用机器学习的方法在一个维度上把脸部的所有特征都纳入训练，这样它可以识别出来这个人的脸部了。但是如果这个人哭了/笑了（面部特征发生了变化），机器可能就识别不出来了。
后来人们提出神经网络算法，就是模仿人类神经元的传递信息的方式。在组合上来说，5+5+5+5往往比10+10 有更加多的组合能力和可能性，这也是一个升维的过程。人们模拟神经元的算法单层感知机只能处理
线性问题，所以需要多个感知器组合处理非线性问题。而当隐藏层大于1层，就是下面我们算法要介绍的多层感知机。


## 多层感知机

多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191207142044607.png)
他类似于人类的神经元结构（树突->细胞体->轴突），多层感知机层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。

在输入层上，这个是我们需要输入的参数x，经过处理特征后的数据

而隐藏层首先它与输入层是全连接的，假设输入层用向量X表示，则隐藏层的输出就是f(W1X+b1)，W1是权重（也叫连接系数），b1是偏置。
隐藏层与隐藏层之间的激活函数f 可以是常用的sigmoid，anh，relu函数

最后就是输出层，其实隐藏层到输出层可以看成是一个多类别的逻辑回归，也即softmax回归，所以输出层的输出就是softmax(W2X1+b2)，X1表示隐藏层的输出f(W1X+b1)。

MLP整个模型就是这样子的，上面说的这个三层的MLP用公式总结起来就是，函数G是softmax，而s是激活函数
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191207151428853.png)
多层感知机依此类推

MLP所有的参数就是各个层之间的连接权重以及偏置，包括W1、b1、W2、b2。对于一个具体的问题，怎么确定这些参数？求解最佳的参数是一个最优化问题，解决最优化问题，最简单的就是梯度下降法了（SGD）：首先随机初始化所有参数，然后迭代地训练，不断地计算梯度和更新参数，直到满足某个条件为止（比如误差足够小、迭代次数足够多时）。这个过程涉及到代价函数、规则化（Regularization）、学习速率（learning rate）、梯度计算等

下一篇说代码实战

参考链接：https://blog.csdn.net/u012162613/article/details/43221829
https://blog.csdn.net/liuyukuan/article/details/72934383
https://blog.csdn.net/u011734144/article/details/80924207