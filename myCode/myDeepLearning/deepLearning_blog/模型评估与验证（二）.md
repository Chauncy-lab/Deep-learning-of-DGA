在前面大致了解模型评估与验证在整个流程所处的地位与作用之后，这一次我们来重点学习分类用的指标含义。
## 1.混淆矩阵
![在这里插入图片描述](https://img-blog.csdnimg.cn/201912191536467.png)
通俗的讲就是：

真实值是positive，模型认为是positive的数量（True Positive=TP）
真实值是positive，模型认为是negative的数量（False Negative=FN）
真实值是negative，模型认为是positive的数量（False Positive=FP）
真实值是negative，模型认为是negative的数量（True Negative=TN）

## 2. 精确率、召回率、准确率、F函数
#### 2.1 精确率和召回率
计算公式如下：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191219154214125.png)
理想情况下，精确率和召回率两者都越高越好。然而事实上这两者在某些情况下是矛盾的，精确率高时，召回率低；精确率低时，召回率高。

precision是相对你自己的模型预测而言：true positive ／retrieved set。假设你的模型一共预测了100个正例，而其中80个是对的正例，那么你的precision就是80%。我们可以把precision也理解为，当你的模型作出一个新的预测时，它的confidence score 是多少，或者它做的这个预测是对的的可能性是多少。

recall是相对真实的答案而言： true positive ／ golden set 。假设测试集里面有100个正例，你的模型能预测覆盖到多少，如果你的模型预测到了40个正例，那你的recall就是40%。

鱼与熊掌不可兼得。如果你的模型很贪婪，想要覆盖更多的sample，那么它就更有可能犯错。在这种情况下，你会有很高的recall，但是较低的precision。如果你的模型很保守，只对它很sure的sample作出预测，那么你的precision会很高，但是recall会相对低。

**两个应用场景**
（1）地震的预测对于地震的预测，我们希望的是RECALL非常高，也就是说每次地震我们都希望预测出来。这个时候我们可以牺牲PRECISION。情愿发出1000次警报，把10次地震都预测正确了；也不要预测100次对了8次漏了两次。
（2）嫌疑人定罪基于不错怪一个好人的原则，对于嫌疑人的定罪我们希望是非常准确的。及时有时候放过了一些罪犯（recall低），但也是值得的。对于分类器来说，本质上是给一个概率，此时，我们再选择一个CUTOFF点（阀值），高于这个点的判正，低于的判负。那么这个点的选择就需要结合你的具体场景去选择。反过来，场景会决定训练模型时的标准，比如第一个场景中，我们就只看RECALL=99.9999%（地震全中）时的PRECISION，其他指标就变得没有了意义。

#### 2.2 准确率
准确率和错误率既可用于二分类也可用于多分类：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191219155506141.png)
精确率和准确率是比较容易混淆的两个评估指标，两者是有区别的。精确率是一个二分类指标，而准确率能应用于多分类

#### 2.3 F1函数
F1函数是一个常用指标，F1值是精确率和召回率的调和均值，即
![在这里插入图片描述](https://img-blog.csdnimg.cn/2019121916002323.png)
## 3. ROC与AUC
#### 3.1 ROC曲线
在众多的机器学习模型中，很多模型输出的是预测概率，而使用精确率、召回率这类指标进行模型评估时，还需要对预测概率设分类阈值，比如预测概率大于阈值为正例，反之为负例。这使得模型多了一个超参数，并且这超参数会影响模型的泛化能力。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20191219160603821.png)
ROC曲线越靠近左上角性能越好


#### 3.1 AUC

AUC是一个模型评价指标，只能用于二分类模型的评价，对于二分类模型，还有很多其他评价指标，AUC是Area under curve的首字母缩写。AUC就是ROC曲线下的面积，衡量学习器优劣的一种性能指标。




参考连接：https://zhuanlan.zhihu.com/p/43405406